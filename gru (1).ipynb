{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import jieba\n",
    "from torch.utils.data import Dataset\n",
    "import itertools\n",
    "import os\n",
    "import datetime\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.ensemble\n",
    "import sklearn.multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devide the validation and test set.\n",
    "tra = pd.read_csv('sentiment_analysis_trainingset.csv')\n",
    "va = pd.read_csv('sentiment_analysis_validationset.csv')\n",
    "allres = tra.append(va)\n",
    "ssall = shuffle(allres)\n",
    "ssall.iloc[:72000,:].to_csv('trainingset_72')\n",
    "ssall.iloc[72000:96000,:].to_csv('valiset_24')\n",
    "ssall.iloc[96000:,:].to_csv('testset_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['location_traffic_convenience', 'location_distance_from_business_district', 'location_easy_to_find',\n",
    "               'service_wait_time', 'service_waiters_attitude', 'service_parking_convenience', 'service_serving_speed',\n",
    "               'price_level', 'price_cost_effective', 'price_discount', 'environment_decoration', 'environment_noise',\n",
    "               'environment_space', 'environment_cleaness', 'dish_portion', 'dish_taste', 'dish_look',\n",
    "               'dish_recommendation',\n",
    "               'others_overall_experience', 'others_willing_to_consume_again']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLoad:\n",
    "    def __init__(self, filename):\n",
    "        word_map = json.load(open(filename, 'r'))\n",
    "        self.word2index = word_map\n",
    "        self.index2word = {v: k for k, v in word_map.items()}\n",
    "        self.n_words = len(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(word_map, c):\n",
    "    return [word_map.get(word, word_map['<unk>']) for word in c] + [word_map['<end>']]\n",
    "\n",
    "def adjust_learning_rate(optimizer, shrink_factor):\n",
    "    print(\"\\nDECAYING learning rate.\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
    "\n",
    "def ensure_folder(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "\n",
    "def accuracy(scores, targets, k=1):\n",
    "    batch_size = targets.size(0)\n",
    "    _, ind = scores.topk(k, 1, True, True)\n",
    "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
    "    # print('correct: ' + str(correct))\n",
    "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "    return correct_total.item() * (100.0 / batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    return datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "min_word_freq = 3\n",
    "batch_every = 50\n",
    "print_every = 100\n",
    "chunk_size = 100\n",
    "num_labels = 20\n",
    "num_classes = 4\n",
    "start_epoch = 0\n",
    "epochs = 10\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "dropout = 0.05\n",
    "batch_first = False\n",
    "save_folder = 'models'\n",
    "\n",
    "assert len(label_names) == 20\n",
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_parse_user_reviews(split):\n",
    "    if split == 'train':\n",
    "        filename = 'trainingset_72'\n",
    "    elif split == 'valid':\n",
    "        filename = 'valiset_24'\n",
    "    else:\n",
    "        #filename = os.path.join(test_a_folder, test_a_filename)\n",
    "        filename = 'testset_24'\n",
    "    user_reviews = pd.read_csv(filename)\n",
    "    return user_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### corpus dictionary\n",
    "def build_wordmap(contents):\n",
    "    word_freq = Counter()\n",
    "\n",
    "    for sentence in tqdm(contents):\n",
    "        seg_list = jieba.cut(sentence.strip())\n",
    "        # Update word frequency\n",
    "        word_freq.update(list(seg_list))\n",
    "\n",
    "    # Create word map\n",
    "    #create frenquency ranking list\n",
    "    words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "    word_map = {k: v + 4 for v, k in enumerate(words)}\n",
    "    word_map['<pad>'] = 0\n",
    "    word_map['<start>'] = 1\n",
    "    word_map['<end>'] = 2\n",
    "    word_map['<unk>'] = 3\n",
    "    print('len(word_map): ' + str(len(word_map)))\n",
    "    print(words[:10])\n",
    "\n",
    "    with open('WORDMAP_train72.json', 'w') as file:\n",
    "        json.dump(word_map, file, indent=4)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    user_reviews = map_parse_user_reviews('train')\n",
    "    build_wordmap(user_reviews['content'])\n",
    "\n",
    "    parse_user_reviews('valid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics_Aver(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "# Exponentially weighted averages\n",
    "class Metrics_ExpoAver(object):\n",
    "    # Exponential Weighted Average Meter\n",
    "    def __init__(self, beta=0.9):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.beta = 0.9\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.val = val\n",
    "        self.avg = self.beta * self.avg + (1 - self.beta) * self.val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "\n",
    "# Meaning    Positive    Neutral    Negative  Not mentioned\n",
    "# Old labels    1\t        0\t        -1\t        -2\n",
    "# New labels    3           2           1           0\n",
    "def map_sentimental_type(value):\n",
    "    return value + 2\n",
    "\n",
    "\n",
    "def parse_user_reviews(user_reviews):\n",
    "    samples = []\n",
    "    for i in range(len(user_reviews)):\n",
    "        content = user_reviews['content'][i]\n",
    "        label_tensor = np.empty((num_labels,), dtype=np.int32)\n",
    "        for idx, name in enumerate(label_names):\n",
    "            sentimental_type = user_reviews[name][i]\n",
    "            y = map_sentimental_type(sentimental_type)\n",
    "\n",
    "            label_tensor[idx] = y\n",
    "        samples.append({'content': content, 'label_tensor': label_tensor})\n",
    "    return samples\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def input_change(indexes_batch):\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def traindata_transfer(pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = input_change(input_batch)\n",
    "    output = torch.LongTensor(output_batch)\n",
    "    return inp, lengths, output\n",
    "\n",
    "### dataset input setup\n",
    "class InputDataset(Dataset):\n",
    "    def __init__(self, split, voc):\n",
    "        self.split = split\n",
    "        self.voc = voc\n",
    "        assert self.split in {'train', 'valid','test'}\n",
    "\n",
    "        if split == 'train':\n",
    "            filename = 'trainingset_72'\n",
    "        elif split == 'valid':\n",
    "            filename = 'valiset_24'\n",
    "        else:\n",
    "            filename = 'testset_24'\n",
    "\n",
    "        user_reviews = pd.read_csv(filename)\n",
    "        self.samples = parse_user_reviews(user_reviews)\n",
    "        self.num_chunks = len(self.samples) // chunk_size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        pair_batch = []\n",
    "\n",
    "        for i_chunk in range(chunk_size):\n",
    "            idx = i * chunk_size + i_chunk\n",
    "            content = self.samples[idx]['content']\n",
    "            content = content.strip()\n",
    "            seg_list = jieba.cut(content)\n",
    "            input_indexes = encode_text(self.voc.word2index, list(seg_list))\n",
    "            label_tensor = self.samples[idx]['label_tensor']\n",
    "            pair_batch.append((input_indexes, label_tensor))\n",
    "\n",
    "        return traindata_transfer(pair_batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        # Initialize LSTM; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_labels * num_classes)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # input_seq = [sent len, batch size]\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # embedded = [sent len, batch size, hidden size]\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        \n",
    "        # outputs = [sent len, batch size, hidden size]\n",
    "        # outputs = outputs[-1]\n",
    "\n",
    "        # Extract the outputs for the last timestep of each example\n",
    "        idx = (input_lengths - 1).view(-1, 1).expand(\n",
    "            len(input_lengths), outputs.size(2))\n",
    "        time_dimension = 1 if batch_first else 0\n",
    "        idx = idx.unsqueeze(time_dimension)\n",
    "        # Shape: (batch_size, rnn_hidden_dim)\n",
    "        outputs = outputs.gather(\n",
    "            time_dimension, Variable(idx)).squeeze(time_dimension)\n",
    "\n",
    "        # outputs = [batch size, hidden size]\n",
    "        outputs = self.fc(outputs)\n",
    "        # outputs = [batch size, num_labels * num_classes]\n",
    "        outputs = outputs.view((-1, num_classes, num_labels))\n",
    "        # outputs = [batch size, num_classes, num_labels]\n",
    "        outputs = F.log_softmax(outputs, dim=1)\n",
    "        # outputs = [batch size, num_classes, num_labels]\n",
    "\n",
    "        # Return output\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0):\n",
    "        super(GRU, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        \n",
    "        self.lstm = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_labels * num_classes)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # input_seq = [sent len, batch size]\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # embedded = [sent len, batch size, hidden size]\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        \n",
    "        idx = (input_lengths - 1).view(-1, 1).expand(\n",
    "            len(input_lengths), outputs.size(2))\n",
    "        time_dimension = 1 if batch_first else 0\n",
    "        idx = idx.unsqueeze(time_dimension)\n",
    "        # Shape: (batch_size, rnn_hidden_dim)\n",
    "        outputs = outputs.gather(\n",
    "            time_dimension, Variable(idx)).squeeze(time_dimension)\n",
    "\n",
    "        # outputs = [batch size, hidden size]\n",
    "        outputs = self.fc(outputs)\n",
    "        # outputs = [batch size, num_labels * num_classes]\n",
    "        outputs = outputs.view((-1, num_classes, num_labels))\n",
    "\n",
    "        outputs = F.log_softmax(outputs, dim=1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttrain(epoch,train_data, encoder,val_data, optimizer):\n",
    "    loss_train_l = []\n",
    "    loss_train_nowl = []\n",
    "    loss_l = []\n",
    "    loss_nowl = []\n",
    "    batch_time = Metrics_Aver()\n",
    "    train_losses = Metrics_ExpoAver()\n",
    "    train_accs = Metrics_ExpoAver()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    for i_batch, (input_variable, lengths, target_variable) in enumerate(train_data):\n",
    "        encoder.train()\n",
    "        criterion =nn.CrossEntropyLoss().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        input_variable = input_variable.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        target_variable = target_variable.to(device)\n",
    "        outputs = encoder(input_variable, lengths)\n",
    "        \n",
    "        encoder.eval()\n",
    "        \n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        for idx, _ in enumerate(label_names):\n",
    "            train_loss += criterion(outputs[:, :, idx], target_variable[:, idx]) / len(label_names)\n",
    "            train_acc += accuracy(outputs[:, :, idx], target_variable[:, idx]) / len(label_names)\n",
    "            \n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.update(train_loss.item())\n",
    "        batch_time.update(time.time() - start)\n",
    "        train_accs.update(train_acc)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        \n",
    "        if i_batch % batch_every ==0:\n",
    "            loss_train_l.append(train_losses.avg)\n",
    "            loss_train_nowl.append(train_losses.val)\n",
    "            \n",
    "            #validation\n",
    "            random_num = int(np.random.randint(0,len(val_data),1))\n",
    "            \n",
    "            for i_batc, (input_variabl, lengts, target_variabl) in enumerate([val_data[random_num]]):\n",
    "            # Set device options\n",
    "                vali_losses = Metrics_Aver()\n",
    "                vali_accs = Metrics_Aver()\n",
    "                input_variabl = input_variabl.to(device)\n",
    "                lengts = lengts.to(device)\n",
    "                target_variabl = target_variabl.to(device)\n",
    "\n",
    "                encoder.eval()\n",
    "                valioutputs = encoder(input_variabl, lengts)\n",
    "\n",
    "                valiloss = 0\n",
    "                valiacc = 0\n",
    "\n",
    "                for idx, _ in enumerate(label_names):\n",
    "                    valiloss += criterion(valioutputs[:, :, idx], target_variabl[:, idx]) / len(label_names)\n",
    "                    valiacc += accuracy(valioutputs[:, :, idx], target_variabl[:, idx]) / len(label_names)\n",
    "                    \n",
    "                vali_losses.update(valiloss.item())\n",
    "                vali_accs.update(valiacc)\n",
    "                loss_l.append(vali_losses.avg)\n",
    "                loss_nowl.append(vali_losses.val)\n",
    "                \n",
    "                \n",
    "        # Print status\n",
    "        if i_batch % print_every == 0:\n",
    "            print('[{0}] Epoch: [{1}][{2}/{3}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Accuracy {accs.val:.3f} ({accs.avg:.3f})'.format(timestamp(), epoch, i_batch, len(train_data),\n",
    "                                                                    batch_time=batch_time,\n",
    "                                                                    loss=train_losses,\n",
    "                                                                    accs=train_accs))\n",
    "    return loss_train_l,loss_train_nowl,loss_l,loss_nowl,vali_accs.avg,train_accs.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training\n",
    "\n",
    "loss_list= []\n",
    "loss_nowlist= []\n",
    "loss_train_list= []\n",
    "loss_train_nowlist=[]\n",
    "voc = LLoad('WORDMAP_train72.json')\n",
    "\n",
    "print(\"voc.n_words: \" + str(voc.n_words))\n",
    "\n",
    "train_data = InputDataset('train', voc)\n",
    "val_data = InputDataset('valid', voc)\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = LSTM(voc.n_words, hidden_size, encoder_n_layers, dropout)\n",
    "\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "best_acc = 0\n",
    "epochs_since_improvement = 0\n",
    "trainaccu_list = []\n",
    "valiaccu_list = []\n",
    "# Epochs\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
    "    if epochs_since_improvement == 20:\n",
    "        break\n",
    "    if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
    "        adjust_learning_rate(optimizer, 0.8)\n",
    "    \n",
    "    loss_train_li,loss_train_nowli,loss_li,loss_nowli ,val_acc,train_acc= ttrain(epoch,train_data, encoder,val_data, optimizer)\n",
    "\n",
    "    is_best = val_acc > best_acc\n",
    "    best_acc = max(best_acc, val_acc)\n",
    "    loss_list.append(loss_li)\n",
    "    loss_nowlist.append(loss_nowli)\n",
    "    loss_train_list.append(loss_train_li)\n",
    "    loss_train_nowlist.append(loss_train_nowli)\n",
    "    if not is_best:\n",
    "        epochs_since_improvement += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "    else:\n",
    "        epochs_since_improvement = 0;\n",
    "    torch.save({'encoder':encoder,'optimizer':optimizer},'model')\n",
    "\n",
    "    \n",
    "    # Reshuffle samples\n",
    "    np.random.shuffle(train_data.samples)\n",
    "    np.random.shuffle(val_data.samples)\n",
    "    trainaccu_list.append(train_acc)\n",
    "    valiaccu_list.append(val_acc)\n",
    "    \n",
    "torch.save(loss_list,'atten_valloss_10epoch_batch50')\n",
    "torch.save(loss_nowlist,'atten_valloss_10epoch_batch50now')\n",
    "torch.save(loss_train_list,'atten_trainloss_10epoch_batch50')\n",
    "torch.save(loss_train_nowlist,'atten_trainloss105epoch_batch50now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([y for x in loss_list for y in x])\n",
    "plt.plot([y for x in loss_train_list for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwe = ['validation_loss','train_loss']\n",
    "plt.figure()\n",
    "plt.plot([y for x in loss_nowlist for y in x])\n",
    "plt.plot([y for x in loss_train_nowlist for y in x])\n",
    "plt.legend(qwe)\n",
    "plt.xlabel('batch step')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('loss_epoch10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw=['training accuracy','validation accuracy']\n",
    "plt.figure()\n",
    "plt.plot(trainaccu_list)\n",
    "plt.plot(valiaccu_list)\n",
    "plt.legend(qw)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim(0,100)\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('accu_epoch10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the accuracy\n",
    "voc = LLoad('WORDMAP_train72.json')\n",
    "test_data = SaDataset('test',voc)\n",
    "#checkpoint = torch.load('model')\n",
    "\n",
    "encoder1 = torch.load('model')\n",
    "encoder11 = encoder1['encoder']\n",
    "\n",
    "encoder11.eval()\n",
    "ac = AverageMeter()\n",
    "\n",
    "for i_batch, (input_variable, lengths, target_variable) in enumerate(test_data):\n",
    "            # Set device options\n",
    "        input_variable = input_variable.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        target_variable = target_variable.to(device)\n",
    "\n",
    "        outputs = encoder11(input_variable, lengths)\n",
    "\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "\n",
    "        for idx, _ in enumerate(label_names):\n",
    "            #loss += criterion(outputs[:, :, idx], target_variable[:, idx]) / len(label_names)\n",
    "            acc += accuracy(outputs[:, :, idx], target_variable[:, idx]) / len(label_names)\n",
    "\n",
    "        # Keep track of metrics\n",
    "        #losses.update(loss.item())\n",
    "        #batch_time.update(time.time() - start)\n",
    "        ac.update(acc)\n",
    "print(ac.avg,'\\n',ac.val )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
